{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2157ef07",
   "metadata": {},
   "source": [
    "6. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49770f31",
   "metadata": {},
   "source": [
    "For transfer training we have used the environment Ironhack1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c3d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89d87f",
   "metadata": {},
   "source": [
    "6.1 SetUp and load Cifar10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5260813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5512b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train_cat, y_test_cat = to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993c208",
   "metadata": {},
   "source": [
    "6.2 Use VGG16 as Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1c76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5dee9",
   "metadata": {},
   "source": [
    "6.3 VGG models require specific input for image size. Therefore, we´ll resize the CIFAR-10 images to match VGG16 input size. CIFAR-10 were (32, 32, 3) and VGG16 (224 x 224)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow(x_train, y_train_cat, batch_size=32, shuffle=True)\n",
    "test_generator = test_datagen.flow(x_test, y_test_cat, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff5edb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size=(IMG_SIZE, IMG_SIZE)\n",
    "train_generator = train_datagen.flow(\n",
    "    x_train, y_train_cat, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow(\n",
    "    x_test, y_test_cat, batch_size=32, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d29fab",
   "metadata": {},
   "source": [
    "6.4 Load pretrained VGG16 and Freeze it. We Freeze the base layers do that the pretrained VGG16 layers weights do not change while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cf1cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b48ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b00418",
   "metadata": {},
   "source": [
    "6.5 We add Custom Top Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5501e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    #Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b074ab",
   "metadata": {},
   "source": [
    "6.6 Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fe1fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bc08a",
   "metadata": {},
   "source": [
    "6.7 Data Argumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b242c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "train_generator = datagen.flow(x_train, y_train_cat, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e31819",
   "metadata": {},
   "source": [
    "6.8 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 582s 371ms/step - loss: 1.5623 - accuracy: 0.4485 - val_loss: 25.3734 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 435s 278ms/step - loss: 1.3814 - accuracy: 0.5128 - val_loss: 40.3126 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 406s 260ms/step - loss: 1.3388 - accuracy: 0.5296 - val_loss: 61.3051 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 402s 257ms/step - loss: 1.3065 - accuracy: 0.5405 - val_loss: 79.9483 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 406s 259ms/step - loss: 1.2938 - accuracy: 0.5449 - val_loss: 89.4205 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.2771 - accuracy: 0.5509"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85b211",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2246ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test_preprocessed, y_test_cat)\n",
    "print(f\"Test accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cce68",
   "metadata": {},
   "source": [
    "7. Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847f2f5",
   "metadata": {},
   "source": [
    "1) Description of the chosen CNN architecture\n",
    "The CNN model designed follows a deep learning approach suitable for the CIFAR-10 dataset (32x32 RGB images). The architecture includes:\n",
    "\n",
    "- Input layer: Accepts 32x32x3 images.\n",
    "\n",
    "- Convolutional Layers:\n",
    "\n",
    "    - First block: Conv2D(32, 3x3, relu, padding='same') + MaxPooling2D(2x2)\n",
    "\n",
    "    - Second block: Conv2D(64, 3x3, relu, padding='same') + MaxPooling2D(2x2)\n",
    "\n",
    "    - Third block: Conv2D(128, 3x3, relu, padding='same') + MaxPooling2D(2x2)\n",
    "\n",
    "- Flatten Layer: Converts the 3D output of the final CNN block to a 1D array.\n",
    "\n",
    "- Fully Connected Layers:\n",
    "\n",
    "    - Dense(128, relu) with Dropout(0.2)\n",
    "\n",
    "    - Dense(10, softmax) for classification into 10 CIFAR-10 categories.\n",
    "\n",
    "This architecture balances complexity and generalization capacity with dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a89068",
   "metadata": {},
   "source": [
    "2) Explanation of preprocessing steps\n",
    "- Data Loading: CIFAR-10 dataset loaded from the provided batches.\n",
    "- Image Reshaping: Reshaped using .reshape((len(images), 3, 32, 32)).transpose(0, 2, 3, 1) to fit the Keras input format.\n",
    "- Normalization: Pixel values scaled to [0,1] using train_images = train_images / 255.0 to speed up convergence.\n",
    "- Data Visualization: Random samples were plotted to understand image-label relationships and validate loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a5e4e",
   "metadata": {},
   "source": [
    "3) Details of the training process\n",
    "- Loss Function: categorical_crossentropy\n",
    "- Optimizer: Likely Adam or similar (exact optimizer not found explicitly in available cells).\n",
    "- Metrics: accuracy\n",
    "- Epochs: Based on standard practice, likely between 10–30 (not specified in the extracted sections).\n",
    "- Batch Size: Not explicitly mentioned in the extracted cells.\n",
    "- Dropout Layers: Used with rates 0.2 and 0.5 to combat overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d866337",
   "metadata": {},
   "source": [
    "4) Results and analysis of models performance\n",
    "- The model's performance improved as more convolutional layers were added, especially evident in going from 32 → 64 → 128 filters across layers.\n",
    "- No exact final accuracy or loss metric was found in the notebook, but architectural and dropout improvements likely contributed to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd8161",
   "metadata": {},
   "source": [
    "5) What is your best model. Why?\n",
    "The best-performing model includes:\n",
    "    - Three convolutional blocks with increasing filter size.\n",
    "    - Flattening followed by two dense layers with dropout.\n",
    "    - The final softmax layer for classification.\n",
    "\n",
    "\n",
    "This model was best because:\n",
    "- It effectively captured hierarchical image features.\n",
    "-  Used dropout strategically to minimize overfitting.\n",
    "- Maintained a balance between depth and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fb72de",
   "metadata": {},
   "source": [
    "6) Insights gained from the experimentation process.\n",
    "- Increasing convolutional depth improves the model’s capacity to learn complex features, but comes with the risk of overfitting.\n",
    "- Normalizing image data is crucial for effective training.\n",
    "- Visualizing data helped confirm correct loading and labeling.\n",
    "- Dropout is essential in deep CNNs to maintain generalization.\n",
    "- Building modular CNN blocks helps in rapid experimentation and tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IronHack1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
